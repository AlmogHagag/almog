{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Mean and standard deviation of all the pixels in the MNIST dataset. They are precomputed \n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "#Transform the images to tensors\n",
    "#Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) \n",
    "#for n channels, this transform will normalize each channel of the input torch.Tensor\n",
    "#i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "transforms_ori = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
    "\n",
    "transforms_photo = transforms.Compose([transforms.Resize((28,28)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
    "\n",
    "#Load our dataset\n",
    "train_dataset = datasets.MNIST(root = './data', \n",
    "                            train = True, \n",
    "                            transform = transforms_ori,\n",
    "                            download = True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root = './data', \n",
    "                            train = False, \n",
    "                            transform = transforms_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdd62a3ffd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "random_image = train_dataset[20][0].numpy() * stddev_gray + mean_gray\n",
    "plt.imshow(random_image.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[20][1].item())   #Print the corresponding label for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the dataset iterable\n",
    "train_load = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} images in the training set'.format(len(train_dataset)))\n",
    "print('There are {} images in the test set'.format(len(test_dataset)))\n",
    "print('There are {} batches in the train loader'.format(len(train_load)))\n",
    "print('There are {} batches in the testloader'.format(len(test_load)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #Same Padding = [(filter size - 1) / 2] (Same Padding--> input size = output size)\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        #The output size of each of the 8 feature maps is \n",
    "        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(28-3+2(1)/1)+1] = 28 (padding type is same)\n",
    "        #Batch normalization\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        #RELU\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        #After max pooling, the output of each feature map is now 28/2 = 14\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #Output size of each of the 32 feature maps remains 14\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        #After max pooling, the output of each feature map is 14/2 = 7\n",
    "        #Flatten the feature maps. You have 32 feature maps, each of them is of size 7x7 --> 32*7*7 = 1568\n",
    "        self.fc1 = nn.Linear(in_features=1568, out_features=600)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=10)\n",
    "    def forward(self,x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before! \n",
    "        #It will take the shape (batch_size, 1568) = (100, 1568)\n",
    "        out = out.view(-1,1568)\n",
    "        #Then we forward through our fully connected layer \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model = model.cuda()    \n",
    "loss_fn = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand what's happening\n",
    "iteration = 0\n",
    "correct_nodata = 0\n",
    "correct_data = 0\n",
    "for i,(inputs,labels) in enumerate (train_load):\n",
    "    if iteration==1:\n",
    "        break\n",
    "    inputs = Variable(inputs)\n",
    "    labels = Variable(labels)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    print(\"For one iteration, this is what happens:\")\n",
    "    print(\"Input Shape:\",inputs.shape)\n",
    "    print(\"Labels Shape:\",labels.shape)\n",
    "    output = model(inputs)\n",
    "    print(\"Outputs Shape\",output.shape)\n",
    "    _, predicted_nodata = torch.max(output, 1)\n",
    "    print(\"Predicted Shape\",predicted_nodata.shape)\n",
    "    print(\"Predicted Tensor:\")\n",
    "    print(predicted_nodata)\n",
    "    correct_nodata += (predicted_nodata == labels).sum()\n",
    "    print(\"Correct Predictions: \",correct_nodata)\n",
    "    _, predicted_data = torch.max(output.data, 1)\n",
    "    correct_data += (predicted_data == labels.data).sum()\n",
    "    print(\"Correct Predictions:\",correct_data)\n",
    "    \n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the CNN\n",
    "num_epochs = 2\n",
    "\n",
    "#Define the lists to store the results of loss and accuracy\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs): \n",
    "    #Reset these below variables to 0 at the begining of every epoch\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
    "        outputs = model(inputs)         \n",
    "        loss = loss_fn(outputs, labels)  \n",
    "        iter_loss += loss.data[0]       # Accumulate the loss\n",
    "        loss.backward()                 # Backpropagation \n",
    "        optimizer.step()                # Update the weights\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(train_dataset)))\n",
    "   \n",
    "    #Testing\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluation mode\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        CUDA = torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        outputs = model(inputs)     \n",
    "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "        loss += loss.data[0]\n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the Testing loss\n",
    "    test_loss.append(loss/iterations)\n",
    "    # Record the Testing accuracy\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n",
    "           .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             test_loss[-1], test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if you want to save the model\n",
    "torch.save(model.state_dict(),'CNN_MNIST.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_accuracy, label='Training Accuracy')\n",
    "plt.plot(test_accuracy, label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if you want to load the model\n",
    "model.load_state_dict(torch.load('CNN_MNIST.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict your own image\n",
    "def predict(img_name,model):\n",
    "    image = cv2.imread(img_name,0)   #Read the image\n",
    "    ret, thresholded = cv2.threshold(image,127,255,cv2.THRESH_BINARY)   #Threshold the image\n",
    "    img = 255-thresholded           #Apply image negative\n",
    "    cv2.imshow('Original',img)      #Display the processed image\n",
    "    cv2.waitKey(0)              \n",
    "    cv2.destroyAllWindows()\n",
    "    img = Image.fromarray(img)      #Convert the image to an array\n",
    "    img = transforms_photo(img)     #Apply the transformations \n",
    "    img = img.view(1,1,28,28)       #Add batch size \n",
    "    img = Variable(img)             #Wrap the tensor to a variable\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        img = img.cuda()\n",
    "\n",
    "    output = model(img)\n",
    "    print(output)\n",
    "    print(output.data)\n",
    "    _, predicted = torch.max(output,1)\n",
    "    return  predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict('3.jpg', model)\n",
    "print(\"The Predicted Label is {}\".format(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
